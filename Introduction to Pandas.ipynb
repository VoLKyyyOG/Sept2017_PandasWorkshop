{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "This workshop will be focused on the exploration, cleaning and basic visualisation of a prepared set of sample data - the Canberra Climate Sensor Data. If you have not already downloaded this data from the [Github](https://github.com/resbaz/Sept2017_PandasWorkshop), please do so now, and place this data file in the same folder as this jupyter notebook. \n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Thorughout this session we're going to be teaching you a range of tools and skills related to cleaning, manipulation and visualising large datasets. Using the pandas package, we can read in and manipulate large spreadsheets of data, and matplotlib lets you visualise these datasets in a useable, customisable format.\n",
    "\n",
    "The three sections I'll be taking you through today are:\n",
    "\n",
    "- Data examination\n",
    "- Dataframe manipulation\n",
    "- Plotting your data with pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "First, we need to import Python's *pandas*, *matplotlib* and *numpy* packages, and then use inline plotting \"magic\" command so that all plots generated will appear within this notebook instead of in a new browser tab.\n",
    "\n",
    "While numpy isn't directly related to this course, it's handy for generating random values, which will be useful when learning how to create your own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Pandas Introduction\n",
    "\n",
    "- creating your own dataframe\n",
    "    - the \"series\" object\n",
    "- subsetting columns\n",
    "- subsetting rows\n",
    "    - `head()` and `tail()`\n",
    "    - slicing\n",
    "    - `loc` vs `iloc`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas dataframe can be thought of as a collection of lists (of equal length), where each list makes up a column inside your dateframe. \n",
    "\n",
    "The key difference between a list and a Pandas column though, is that every single item inside your column _must be of the same data type_. If you have a column of integers, and a single string value, like this, `[1,2,3,4,'seven']`, then every single value inside your column is going to be a string-type.\n",
    "\n",
    "Instead of using a list - which can take any type of values -, Pandas performs this type-coercion by using a data type called a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each \"Series\" object can be thought of as it's own miniature dataframe. So our previous example would be a dataframe with one column, and 5 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when creating a dataframe, you actually have to create it as a collection of these \"Series\" objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "         {'A':['a', 'b', 'c','d','e','f'], \n",
    "             'B':[54, 67, 89, 100, None, 64],\n",
    "             'C': np.random.randn(6),\n",
    "             'D': [2.6,None, 8.0, 9.4, 3.3, None]\n",
    "                },\n",
    "          index=[49, 48, 47, 1, 2, 3] # Lets you set your row names\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data\n",
    "\n",
    "The first step to any data exploration and manipulation is to open your data within your program. We are going to do this using the **pandas** package, which reads in spread-sheet style data and converts them into *dataframes*. \n",
    "\n",
    "These dataframes work with rows and columns, like a spreadsheet, except that all data within a single column has to be the same data type. \n",
    "\n",
    "For example, imagine you had a spreadsheet containing two columns - \"labels\" and \"numbers\", and that the rows in the \"labels\" column contains either a text or number sequence. Because you cannot turn text into a number, every single row in that \"labels\" column would need to be a string (text) type. Similarly, if some (but not all) of the rows in the \"numbers\" column contained decimals, **all** of the rows within this column would need to be of a decimal (float) data type.\n",
    "\n",
    "To read in a comma-separated file, or \\*.csv, you can use the pandas function read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_observations = pd.read_csv('Canberra_observations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also open a variety of other file types using the \"reader\" functions found in this [IO tools documentation](http://pandas.pydata.org/pandas-docs/version/0.20/io.html \"Pandas IO tools\"). \n",
    "\n",
    "This includes file types such as excel (\\*.xlsx) files, and text (\\*.txt) files. You can use the parameters within these functions to specify file or data attributes such as column separators, whether there's column/row names, and even specifying your own column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Seems as though this file is tab separated, not comma separated. \n",
    "\n",
    "Fortunately, pandas.read_csv() has a range of keyword arguments to read in our data.\n",
    "\n",
    "- `sep`: the type of separator between our columns\n",
    "-  `header`: the row number to take as the start of the data. Useful if you have metadata attached at the beginning of your file\n",
    "- `names`: You can also specify your column names. Takes a list of values. If your file contains no header, also use `header = none`. Otherwise, use `header = 0`.\n",
    "* `parse_dates`: Treat one or more columns like dates.\n",
    "* `dayfirst`: Use DD.MM.YYYY format, not month first.\n",
    "* `infer_datetime_format`: Tell pandas to guess the date format.\n",
    "- `na_values`: Specify values to be treated as empty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weather_observations = pd.read_csv('Canberra_observations.csv',\n",
    "                                   sep='\\t',\n",
    "                                   na_values=['-'],\n",
    "                                   parse_dates={'Datetime': ['Date', 'Time']},\n",
    "                                   dayfirst=True, #redundant in this instance\n",
    "                                   infer_datetime_format=True, #redundant in this instance\n",
    "                                  )\n",
    "# Display some entries\n",
    "weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining your Data\n",
    "\n",
    "- `head()`, `tail()`\n",
    "- `columns`\n",
    "- `df.Column` vs `df['Column']`\n",
    "- `dtypes`\n",
    "- `describe()`\n",
    "- `shape`; `shape[0]` vs. `shape[1]`\n",
    "- `iloc` vs `loc`\n",
    "\n",
    "One of the first steps in exploring your data is to see what it looks like, what data types are present, and how many rows/columns there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_observations.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify how many rows you want `head` and `tail` to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` function gives you the dimensions of your data, in the form `(#rows, #columns)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a tuple (or linked pairs) of the number of rows and columns in your dataframe.\n",
    "weather_observations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that we have 12 columns, and 19,918 rows of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the first or second element of the tuple can give you either the rows or the columns\n",
    "#Gives you the rows (remember, 0 indexing!)\n",
    "print(weather_observations.shape[0])\n",
    "\n",
    "#Gives you the columns\n",
    "print(weather_observations.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `df.columns` to examine the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also examine the data types inside your columns using `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What an \"object\" data type??\n",
    "weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting Columns\n",
    "\n",
    "To select a particular column in your dataframe, you can use one of two options:\n",
    "\n",
    "1) Calling the column as an \"attribute\"\n",
    "    - `df.columnName`\n",
    "\n",
    "2) Subsetting the dataframe\n",
    "    - `df['columnName']`\n",
    "\n",
    "The first option is useful for some functions, but the second form is essential if you want to call more than column at once. You do this by inserting a list, [], of column names, instead a single column.\n",
    "\n",
    "For example: `df[[\"Column1\", \"Column2\", ... , etc]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.Tmp.head() #you can also \"chain\" commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations[\"Wind dir\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try subsetting the first two columns\n",
    "weather_observations[[\"Datetime\",\"Wind dir\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you change the order of the columns around?\n",
    "weather_observations[[\"Wind dir\",\"Datetime\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about a column that doesnt exist?\n",
    "weather_observations.Wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing\n",
    "\n",
    "Sometimes you need to examine specific rows and columns in the middle of your data though, which aren't covered by `head()` or `tail()`. Instead, you can use index slicing.\n",
    "\n",
    "Slicing works similarly to how you might slice a string, or a list. You simply call the indexes of the rows you want from the dataframe: `df[rowNumbers]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations[[\"Pres\",\"rh\",\"Fire\"]][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `loc` vs `iloc`\n",
    "\n",
    "You can also use `loc` and `iloc` to slice rows and columns.\n",
    "\n",
    "`df.iloc[]` is positional based, so takes integer values that correlate with the row and column **numbers** in your dataframe\n",
    "\n",
    "`df.loc[]` is label based, and takes the row and column **names** as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we're going to use our toy dataframe, df, as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using iloc to get rows 0, 1 and 2\n",
    "df.iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using loc to get row labels 1, 2 and 3\n",
    "df.loc[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only enter one set of values into `loc` and `iloc`, they will return the values for every column in your dataset.\n",
    "\n",
    "By using a second integer though, you can choose which rows and columns you specifically want to subset. The first value corresponds to the row, and the second to the columns, or rows x columns. You can also think of this with the moniker *\"Roman Catholic\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using loc to get row labels 1, 2 and 3 for Columns A and B\n",
    "df.loc[1:3,[\"A\",\"B\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using iloc to get rows 0, 1 and 2 for columns 0 and 1.\n",
    "df.iloc[:3,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between `loc` and `iloc` can seem minor, but they're very important, and which one you should use depends on what your needs are at the time.\n",
    "\n",
    "`iloc` is based on dataframe position, so calling `iloc[:3]` would give you rows 0 through 3. \n",
    "\n",
    "`loc` however is based on the index label, so if you were to call `df.loc[:3]`, it would give you all rows UP TO the row labelled as index 3.\n",
    "\n",
    "While the indexes are in order and all present, this isn't an issue. Consider what happens when the indexes are out of order though, with our dataframe 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see that this only takes the first 2 rows\n",
    "df.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas this takes all rows UP TO index label 2\n",
    "df.loc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `loc` is based on labels, if you try to subset a row or column label that doesn't exist, even if it corresponds to a positional row, python will throw you an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this, it's important that you carefully consider which tool is appropriate for your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Find the 11th through 20th (inclusive) rows for the columns related to wind variables using each of loc, iloc and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#slicing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting with Conditionals\n",
    "\n",
    "You can also subset using conditional statements, such as ==, !=, >, <, etc.\n",
    "\n",
    "For example, if I want to find all of the rows where Wind Gust > 80, I would type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns/rows where the wind gusts > 80\n",
    "weather_observations[weather_observations[\"Wind gust\"] > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with lists and for loops, etc, you can also combine these conditionals using & {and} , or | {or}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Find the rows where Wind spd < 10 AND Pres > 1020\n",
    "weather_observations[(weather_observations[\"Wind spd\"] > 60) & (weather_observations.Pres > 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also get a list of the row indexes for your subset\n",
    "list(weather_observations[(weather_observations[\"Wind spd\"] > 60) & (weather_observations.Pres > 1000)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can subset using boolean lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a boolean list. The 1st, 3rd and 5th values are True.\n",
    "na = weather_observations[\"Wind spd\"].isnull()\n",
    "\n",
    "weather_observations[na] #subsets all rows where Wind spd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "How many rows exist where null values are concurrently present in the columns \"Wind dir\", \"Fire\" and \"Wind spd\"?\n",
    "\n",
    "_Hint: Remember that you can combine conditions with '&'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describing your data\n",
    "\n",
    "Often you'll also want a quick summary of your data and what's in each column\n",
    "\n",
    "You can do this with the function `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it's excluded the non-numeric columns though.\n",
    "\n",
    "To get a description of all of your data at once, you need to give `describe` the `include = all` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find specific statistics for each of these columns by using `max()`, `min()`, `count()`, `std()` {standard deviation}, `mean()` and `sum()`. \n",
    "\n",
    "Just remember though that many of these functions rely on numeric data types, and will cause errors if used on a str type. Calling `sum()` on a string however will concatentate those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The maximum value in Fire\n",
    "weather_observations.Fire.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of null values in Fire\n",
    "weather_observations.Fire.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try finding the maximum of the Location (a string) column\n",
    "weather_observations['Wind dir'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to know exactly what and how many different values you have in categorical columns.\n",
    "\n",
    "`unique()` works on both numeric and string data, and gives you a list of all of the unique values within a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations[\"Wind dir\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the size/how many unique values there in two ways:\n",
    "\n",
    "1) Taking the `len()` of the array  \n",
    "2) Using the numpy `size` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Len:\", len(weather_observations[\"Wind dir\"].unique()))\n",
    "\n",
    "print(\"Numpy:\", weather_observations[\"Wind dir\"].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Manipulating Data\n",
    "\n",
    "- Dealing with duplicates\n",
    "- Sorting data\n",
    "- using the `apply()` function\n",
    "    - `reset_index` for row names\n",
    "- Adding and deleting columns/rows\n",
    "    - renaming columns\n",
    "- setting data frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at our data you can notice some funny things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 and last 5 values around the 01/01/2013\n",
    "weather_observations[\"Datetime\"].iloc[list(range(5)) + list(range(48,53))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does it order the time series in the reverse order, but it actually includes two midnight values for the same day - one at the start and one at the end of each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicated items with the same date and time\n",
    "no_duplicates = weather_observations.drop_duplicates('Datetime', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_observations.shape)\n",
    "print(no_duplicates.shape)\n",
    "\n",
    "no_duplicates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `sort_values()` to sort our data in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting is ascending by default, or chronological order\n",
    "sorted_dataframe = no_duplicates.sort_values('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, while querying our data, it might be better to have our datetime as our indexes, rather than some arbitrary numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use `Datetime` as our DataFrame index\n",
    "indexed_weather_observations = sorted_dataframe.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you didn't want to index your data with another column, you can just do `df.reset_index()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and deleting columns\n",
    "\n",
    "#### The `apply()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to model this data, we would need the `Wind  dir` column to be numeric values, rather than strings.\n",
    "\n",
    "Thankfully, we can do this using the \"apply\" function.\n",
    "\n",
    "Apply will essentially \"apply\" a function of your choosing to the specified column/s. This could be to create a new column based off of the values of other columns, or, as in this case, processing the data within a single column to become something new.\n",
    "\n",
    "In this particular instance, each of our wind directions corresponds to an angle:\n",
    " * North wind (↓) is 0 degrees, going clockwise ⟳.\n",
    " * East wind (←) is 90 degrees.\n",
    " * South wind (↑) is 180 degrees\n",
    " * West wind (→) is 270 degrees\n",
    " * etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate wind direction to degrees\n",
    "wind_directions = {\n",
    "     'N':   0. , 'NNE':  22.5, 'NE':  45. , 'ENE':  67.5 ,\n",
    "     'E':  90. , 'ESE': 112.5, 'SE': 135. , 'SSE': 157.5 ,\n",
    "     'S': 180. , 'SSW': 202.5, 'SW': 225. , 'WSW': 247.5 ,\n",
    "     'W': 270. , 'WNW': 292.5, 'NW': 315. , 'NNW': 337.5 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to create a new column, called \"Wind deg\", we simply need to assign a new \"series\" object to a new column in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new wind directions column with a new number column\n",
    "# `get()` accesses values safely from dictionary\n",
    "\n",
    "# Create a new column \n",
    "indexed_weather_observations['Wind deg'] = \\\n",
    "    indexed_weather_observations['Wind dir'].apply(wind_directions.get) # using these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could have just over-written the current values inside `Wind dir` instead of creating a new column.\n",
    "\n",
    "However, now that we have a new column, we want to delete the old one. \n",
    "\n",
    "Just as with a list, we can do this with the command `del`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del indexed_weather_observations[\"Wind dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_weather_observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you then wanted to reorder your columns\n",
    "indexed_weather_observations = indexed_weather_observations.iloc[:,[-1]+list(range(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming columns\n",
    "If I wanted, we could also rename our `Wind deg` column to be `Wind dir` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_weather_observations =\\\n",
    "            indexed_weather_observations.rename(columns = {'Wind deg':'Wind dir'})\n",
    "\n",
    "indexed_weather_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Create a new column \"E\" for our toy dataframe, df. \"E\" will contain the values from column B + 100. \n",
    "\n",
    "_Hint 1: You can test your output by using the \"apply\" function without assigning it to a new column first_\n",
    "\n",
    "_Hint 2: you can create your own functions using \"lambda\". e.g. _`apply(lambda x: ` _`insert stuff with x`_`)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Frequency\n",
    "\n",
    "If you delve into your data, you might occasionally see some timestamps that don't follow the typical half-hour format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One section where the data has weird timestamps ...\n",
    "indexed_weather_observations[1800:1806]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function, `df.asfreq()`, allows you to force a frequency on your index, and will discard/fill in the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the index to be every 30 minutes\n",
    "regular_observations = indexed_weather_observations.asfreq('30min')\n",
    "\n",
    "print(regular_observations.shape) # we've now deleted ~ 2000 rows\n",
    "\n",
    "# Same section we observed earlier\n",
    "regular_observations[1633:1638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data\n",
    "\n",
    "- finding missing data\n",
    "- interpolating missing data\n",
    "- data interrogation with the `groupby` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to know where and if we have any missing data in our dataset\n",
    "\n",
    "A rudimentary way of doing this is with `counts`. `counts` will give you the number of **non-null** values within each column. Therefore, if your columns are shorter than the number of rows, you have nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_observations.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here, none of our columns contain all 17520 values, with the most significant number of nulls being found in \"Wind dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen when applying conditional statements before, there is also the `isnull()` function, which returns a list of True/False values. This can be used to subset your data to find the null rows, or to count how many there are in specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_observations[\"Wind dir\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also show *every* row with null values at once using the isnull().any() chained operation. any() contains the optional parameter \"axis\", which allows you to choose whether it operates on the rows or the columns. By default axis = 0, which gives you columns, but setting axis = 1 checks over the rows instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will give the total number of rows that contain nulls\n",
    "regular_observations.isnull().any(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will give True/False values for whether a column contains nulls\n",
    "regular_observations.isnull().any(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we plot our data we can see that we have a significant portion of data missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the graphs a bit prettier\n",
    "# pd.set_option('display.mpl_style', 'default') \n",
    "# plt.rcParams['figure.figsize'] = (18, 5)\n",
    "\n",
    "regular_observations[['Wind spd', 'Wind gust', 'Tmp', 'Feels like']][:500].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we were seeing while tracking the null values, we can see that there are a number of values missing even just in January\n",
    "\n",
    "This can be due to errors in the sensors, or because of maintainence, etc. However if we wanted to model all of this data, this data needs to be complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating Missing Data\n",
    "\n",
    "One option is to use the function `Series.interpolate` to predict and fill in the missing values based on the index\n",
    "\n",
    "If we examine the function more closely, we can see that it can take multiple arguments, including the prediction method (default = linear), and the direction in which it can replace your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df.interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the null values    \n",
    "regular_observations[1633:1638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interpolate data to fill empty values\n",
    "for column in regular_observations.columns:\n",
    "    regular_observations[column].interpolate('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The null values have been replaced    \n",
    "regular_observations[1633:1638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it again - to be sure\n",
    "regular_observations[['Wind spd', 'Wind gust', 'Tmp', 'Feels like']][:500].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No gaps!\n",
    "\n",
    "If you didn't want to predict values, there are a few other options you have to resolve nulls.\n",
    "\n",
    "One option, provided you don't require a consecutive sequence of data, is to delete your null observations.\n",
    "\n",
    "This can be easily done using the `dropna()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dropna()` can work on either the rows or the columns, and allows you to specify whether you want to remove rows/column where either 'any' or 'all' of the data are nulls. Alternatively, you can set a threshold value, where rows/columns are discarded if they don't have at least a threshold # of non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df = indexed_weather_observations.dropna(axis = 0, thresh= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indexed_weather_observations.shape)\n",
    "\n",
    "temp_df.shape #As we can see, only 11 rows had < 5 values in their row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we could replace all the null values in each column with an appropriate value. While this wouldn't be appropriate for this dataset, there are many where a default value would be appropriate. You can do this with a method called `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"E\"] = df[\"E\"].fillna(value = 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fillna` also allows you to forward or backfill your dataframe with the \"methods\" argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"B\"] = df[\"B\"].fillna(method = 'ffill')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"D\"] = df[\"D\"].fillna(method = 'bfill')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data\n",
    "\n",
    "It's often useful to be able to group the data within a column according to the data in another column. \n",
    "\n",
    "For example, you might wish to take the mean temperature for each month, or each year. We could do this in a complicated and time consuming way where you subset the data for each month, and then take the mean of the Tmp column...or we could just use the `groupby` function. `groupby` outputs a reformatted version of your data where all values associated with your \"grouping factor\" are taken together. You can then perform basic statistical tests (or plot) on this grouped output, and the tests will be performed within each of the \"groups\" you defined.\n",
    "\n",
    "Going back to the previous example, say that we wanted to find the mean temperature seen in each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use TimeGrouper because we're grouping based on datetime data\n",
    "regular_observations[[\"Tmp\",\"Feels like\"]].groupby(pd.TimeGrouper(freq='M')).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to group over multiple factors, such as by Month and Year together, by specifying a list for the `by` parameter within `groupby()`. The order of the values in this list matters, as `groupby` will first group your data based on the first factor, and THEN the second factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grouping by month, then by day\n",
    "#The average number of pedestrians \n",
    "regular_observations[[\"Wind spd\", \"Wind gust\"]].groupby(by=\\\n",
    "                                            [pd.TimeGrouper(freq='M'),pd.TimeGrouper(freq='D')]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain together more commands, like max(), etc, to find more specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum average monthly temperature \n",
    "regular_observations['Tmp'].groupby(by=pd.TimeGrouper(\"M\")).mean().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If i wanted to find which month that belonged to though....\n",
    "max_temp = regular_observations['Tmp'].groupby(by=pd.TimeGrouper(\"M\")).mean().max()\n",
    "\n",
    "regular_observations[\"Tmp\"].groupby(by=pd.TimeGrouper(\"M\")).mean()[\\\n",
    "                                    regular_observations[\"Tmp\"].groupby(by=pd.TimeGrouper(\"M\")).mean() == max_temp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Which month has the greatest standard deviation in temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Data\n",
    "\n",
    "Pandas has some in-built plotting functionality, that builds off of the traditional matplotlib library\n",
    "\n",
    "We've already seen a bit of this with \"plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_observations[['Wind spd', 'Wind gust', 'Tmp', 'Feels like']][:500].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_observations[:500].plot(x = \"Feels like\", y = \"Tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " scatter_matrix(regular_observations[:500], alpha=0.2, diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import andrews_curves\n",
    "\n",
    "plt.figure()\n",
    "andrews_curves(regular_observations[:500], 'Wind dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "parallel_coordinates(regular_observations[:500], 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see exampes of these and many more inside the pandas documentation, https://pandas.pydata.org/pandas-docs/stable/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have another dataset, called \"Canberra Sky\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Read in the Canberra Sky file, as `sky_observations`, in an appropriate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the \"head\" of the data, tt seems to have the same issues that our previous dataset originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, remove duplicates and set index to datetime.\n",
    "sky_observations.drop_duplicates('Datetime', keep='last', inplace=True)\n",
    "sky_observations.sort_values('Datetime', inplace=True)\n",
    "sky_observations.set_index('Datetime', inplace=True)\n",
    "\n",
    "sky_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Remove all of the rows in `sky_observations` that have no data in them. i.e. all observations for that timestamp are NaN\n",
    "\n",
    "_Hint: remember the `dropna` function?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's explore our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the inferred data types\n",
    "sky_observations.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer look at Cloud shows that, rather than restricting the column values to using a scale between 1 - 8, people have been typing in a fractional string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the inferred data types\n",
    "sky_observations.Cloud.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Cloud column, 'clear' means no clouds, and 'nan' represents \"obscured\", meaning that the visibility was too low to see cloud. In this case, we would take that as a null value\n",
    "\n",
    "To convert this column into a numerical value, we can define our own function and then use `apply` just as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to Change the 'Cloud' column to numerical values\n",
    "def cloud_to_numeric(s):\n",
    "    if s == 'clear' or pd.isnull(s):\n",
    "        return 0\n",
    "    else:\n",
    "        # s[0] is the first str character, or the numeric value\n",
    "        return int(s[0]) / 8.0 # because we want a scale between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to every item and assign it back to the original dataframe\n",
    "sky_observations['Cloud'] = \\\n",
    "    sky_observations['Cloud'].apply(cloud_to_numeric, convert_dtype=False).astype('float64')\n",
    "sky_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a cleaned dataset for `sky_observations` that only contain numeric values. \n",
    "\n",
    "If we wanted to combine this with our previous `regular_observations` dataset though, we can see that despite `sky_observations` being indexed over the same time period as `regular_observations`, we had to remove many more of the null values, and it now has far fewer rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sky_observations.shape)\n",
    "\n",
    "print(regular_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `combine` function\n",
    "Therefore if we wanted to add these two datasets together, we couldn't use the simple Column addition that we learned earlier.\n",
    "\n",
    "To combine these wo datasets, we can therefore use the function `df1.combine(df2)`\n",
    "\n",
    "In this particular instance, we only want to join the Cloud column to our `regular_observations` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two observations together\n",
    "# combined_observations = regular_observations.combine_first(sky_observations[['Cloud']])\n",
    "combined_observations.head()\n",
    "# combined_observations[combined_observations.Cloud.notnull()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of null values, but it joined it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Try joining another column from `sky_observations` onto the `combined_observations` dataframe. Which column/s would be the most appropriate? Which would be the least? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This session has taken you through the beginner's guide of how to visualise, clean and investigate your data, and given you the basic tools to query your own research data. \n",
    "\n",
    "Python is a very versatile tool, and can go much further than what we've shown you today. Many packages are open source and being developed for a range of purpoess all the time. Scipy allows you to perform basic math and statistical tests for example, there are a host of packages related to investigating biological data.\n",
    "\n",
    "Using a programming language allows you to investigate much larger data than you can do by hand or by eye, and the customisability of the plotting tools makes it ideal for generating useful and professional images  ideal for publication.\n",
    "\n",
    "Follow us on Twitter to keep up-to-date on new and up-coming trainings\n",
    "- [@kflekac](https://twitter.com/kflekac)\n",
    "- [@ResBaz](https://twitter.com/resbaz)\n",
    "- [@ResPlat](https://twitter.com/resplat)\n",
    "\n",
    "There's also a reasonably new facebook group, called [\"Data Wrangling with Python\"](https://www.facebook.com/groups/797677037064561/) where you can post your python problems, cool things you've done, and stay apprised of new trainings coming up as well.\n",
    "\n",
    "Also remember that if you're having problems, Research Platforms runs a weekly Hacky Hour, where anybody and everybody is able to enquire about coding and programming problems in a variety of tools. Every Thursday from 3-4pm, at the large table in Tsubu Bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "532px",
    "left": "0px",
    "right": "1141px",
    "top": "106px",
    "width": "225px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
